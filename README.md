# üß† DeepSeek Components ‚Äì Illustrated in Python

This repository is a collection of **Kaggle Notebooks**, each focused on implementing and explaining **one specific component** of transformer architectures, with a strong emphasis on **DeepSeek**-style enhancements.

## üìå What is This Repo About?

DeepSeek is a cutting-edge, open-source language model project that builds upon the foundations of Transformers with improved efficiency, scalability, and training techniques.

This repo serves two goals:

1. **Learn the Basics** ‚Äì Implement foundational transformer components (like attention, layer norm, etc.) from scratch using Python.
2. **Explore DeepSeek‚Äôs Improvements** ‚Äì Highlight and explain how DeepSeek improves upon standard transformers with practical examples.

## üìò How to Use

Each **Kaggle Notebook** in this repository:

* Focuses on a single module or concept (e.g., Rotary Embedding, Efficient Attention).
* Explains **how it works** with clean Python code.
* Demonstrates usage through small-scale tests or visualizations.

You can use this repo as:

* A **learning guide** to understand transformer internals.
* A **reference** to see how DeepSeek differs from standard models like GPT.
* A **codebase** for building or extending your own models.

## üóÇÔ∏è Notebook Structure current

| Notebook                        | Description                                                                                                       |
| ------------------------------- | ------------------------------------------------------------------------------------------------------------------|
| `01_multi_head_attention.ipynb` | Builds multi-head attention from scratch also include implementation of self attention mechanism, causal attentio |
| `...`                           | More to come: weight tying, training tricks, etc.                                                                 |

## üí° Why This Format?

* **Notebook-based:** Great for visual learners ‚Äî explanations and code go hand-in-hand.
* **Component-wise:** Easy to focus on one idea at a time.
* **DeepSeek-focused:** Brings theory into practice using modern architecture patterns.

## üìö References

* [DeepSeek Project](https://deepseek.com/)
* [Transformer Paper: Attention is All You Need](https://arxiv.org/abs/1706.03762)
* RoPE (Rotary Positional Embedding)
* FlashAttention, Fused Kernels, and other modern tricks

---

Feel free to clone, run, and tweak the notebooks as you learn!

